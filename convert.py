import os
import shutil
from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TFAutoModelForSequenceClassification
import torch
import tensorflow as tf
import numpy as np

def convert_safetensors_to_tflite(
    model_dir="./model",
    tflite_output_path="./model.tflite",
    max_length=128
):
    """
    Convertit un mod√®le SafeTensors en TFLite en passant par TensorFlow directement
    
    Args:
        model_dir: Dossier contenant model.safetensors + config.json
        tflite_output_path: Chemin de sortie pour le mod√®le TFLite
        max_length: Longueur maximale des s√©quences
    """
    
    try:
        # === 1. Nettoyage des dossiers pr√©c√©dents ===
        tf_model_dir = "./tf_model"
        if os.path.exists(tf_model_dir):
            shutil.rmtree(tf_model_dir)
            print(f"üóëÔ∏è Dossier {tf_model_dir} supprim√©")
        
        # === 2. Charger le mod√®le PyTorch ===
        print("üì• Chargement du mod√®le PyTorch depuis les fichiers locaux (.safetensors)...")
        config = AutoConfig.from_pretrained(model_dir, local_files_only=True)
        pytorch_model = AutoModelForSequenceClassification.from_pretrained(
            model_dir, 
            config=config, 
            local_files_only=True
        )
        tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)
        
        print(f"‚úÖ Mod√®le PyTorch charg√© : {config.model_type}")
        print(f"üìä Nombre de labels : {config.num_labels}")
        
        # === 3. Cr√©er le mod√®le TensorFlow √©quivalent ===
        print("üîÑ Cr√©ation du mod√®le TensorFlow...")
        
        # Cr√©er le mod√®le TF avec la m√™me configuration
        tf_model = TFAutoModelForSequenceClassification.from_config(config)
        
        # === 4. Transf√©rer les poids de PyTorch vers TensorFlow ===
        print("üîÑ Transfert des poids PyTorch vers TensorFlow...")
        
        # Pr√©parer des donn√©es factices pour initialiser le mod√®le TF
        dummy_inputs = {
            'input_ids': tf.constant([[1, 2, 3, 4, 5]], dtype=tf.int32),
            'attention_mask': tf.constant([[1, 1, 1, 1, 1]], dtype=tf.int32)
        }
        
        # Faire un passage avant pour initialiser les couches
        _ = tf_model(dummy_inputs)
        
        # Transf√©rer les poids manuellement
        pytorch_state_dict = pytorch_model.state_dict()
        
        # Cr√©er un mapping des poids PyTorch vers TensorFlow
        weight_mapping = create_weight_mapping(pytorch_model, tf_model)
        
        for tf_var, pt_param_name in weight_mapping.items():
            if pt_param_name in pytorch_state_dict:
                pt_weight = pytorch_state_dict[pt_param_name].detach().numpy()
                
                # Ajuster la forme si n√©cessaire (pour les couches lin√©aires)
                if len(pt_weight.shape) == 2 and len(tf_var.shape) == 2:
                    if pt_weight.shape != tf_var.shape:
                        pt_weight = pt_weight.T
                
                try:
                    tf_var.assign(pt_weight)
                    print(f"‚úÖ Poids transf√©r√©s : {pt_param_name} -> {tf_var.name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur transfert {pt_param_name}: {e}")
        
        # === 5. Cr√©er un mod√®le avec signature concr√®te ===
        print("üîß Cr√©ation du mod√®le avec signature concr√®te...")
        
        @tf.function
        def serving_fn(input_ids, attention_mask):
            outputs = tf_model({
                'input_ids': input_ids,
                'attention_mask': attention_mask
            })
            return outputs.logits
        
        # D√©finir la signature concr√®te
        concrete_function = serving_fn.get_concrete_function(
            input_ids=tf.TensorSpec(shape=[None, max_length], dtype=tf.int32, name='input_ids'),
            attention_mask=tf.TensorSpec(shape=[None, max_length], dtype=tf.int32, name='attention_mask')
        )
        
        # === 6. Sauvegarder le mod√®le TensorFlow ===
        print("üíæ Sauvegarde du mod√®le TensorFlow...")
        tf.saved_model.save(
            tf_model,
            tf_model_dir,
            signatures={'serving_default': concrete_function}
        )
        print(f"‚úÖ Mod√®le TensorFlow sauvegard√© : {tf_model_dir}")
        
        # === 7. Conversion vers TFLite ===
        print("üîÑ Conversion vers TFLite...")
        converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)
        
        # Configuration du convertisseur
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS,
            tf.lite.OpsSet.SELECT_TF_OPS  # Permet d'utiliser certaines ops TF non support√©es nativement
        ]
        
        # Fonction repr√©sentative pour la quantification (optionnelle)
        def representative_dataset():
            for _ in range(100):
                # Cr√©er des donn√©es al√©atoires repr√©sentatives
                input_ids = np.random.randint(1, 1000, size=(1, max_length), dtype=np.int32)
                attention_mask = np.ones((1, max_length), dtype=np.int32)
                yield [input_ids, attention_mask]
        
        # Essayer la conversion avec optimisations
        try:
            converter.representative_dataset = representative_dataset
            tflite_model = converter.convert()
            print("‚úÖ Conversion avec optimisations r√©ussie")
        except Exception as e:
            print(f"‚ö†Ô∏è √âchec avec optimisations: {e}")
            print("üîÑ Tentative sans optimisations...")
            
            # Nouvelle tentative sans optimisations
            converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.TFLITE_BUILTINS,
                tf.lite.OpsSet.SELECT_TF_OPS
            ]
            tflite_model = converter.convert()
            print("‚úÖ Conversion sans optimisations r√©ussie")
        
        # === 8. Sauvegarde du mod√®le TFLite ===
        with open(tflite_output_path, "wb") as f:
            f.write(tflite_model)
        
        # Informations sur le mod√®le final
        model_size = os.path.getsize(tflite_output_path) / (1024 * 1024)  # MB
        print(f"‚úÖ Conversion termin√©e !")
        print(f"üìÅ Mod√®le TFLite : {tflite_output_path}")
        print(f"üìè Taille du mod√®le : {model_size:.2f} MB")
        
        # === 9. Test du mod√®le TFLite ===
        print("üß™ Test du mod√®le TFLite...")
        test_tflite_model(tflite_output_path, tokenizer, max_length)
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la conversion : {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def create_weight_mapping(pytorch_model, tf_model):
    """
    Cr√©e un mapping entre les poids PyTorch et TensorFlow
    """
    weight_mapping = {}
    
    # Obtenir tous les param√®tres nomm√©s
    pt_params = dict(pytorch_model.named_parameters())
    tf_vars = {var.name: var for var in tf_model.trainable_variables}
    
    # Mapping de base pour BERT
    base_mappings = {
        'embeddings.word_embeddings.weight': 'embeddings/word_embeddings/weight:0',
        'embeddings.position_embeddings.weight': 'embeddings/position_embeddings/weight:0',
        'embeddings.token_type_embeddings.weight': 'embeddings/token_type_embeddings/weight:0',
        'embeddings.LayerNorm.weight': 'embeddings/layer_norm/gamma:0',
        'embeddings.LayerNorm.bias': 'embeddings/layer_norm/beta:0',
        'classifier.weight': 'classifier/weight:0',
        'classifier.bias': 'classifier/bias:0',
    }
    
    # Ajouter les mappings pour les couches d'attention et feed-forward
    for i in range(12):  # BERT-base a g√©n√©ralement 12 couches
        layer_mappings = {
            f'encoder.layer.{i}.attention.self.query.weight': f'encoder/layer_._{i}/attention/self/query/weight:0',
            f'encoder.layer.{i}.attention.self.query.bias': f'encoder/layer_._{i}/attention/self/query/bias:0',
            f'encoder.layer.{i}.attention.self.key.weight': f'encoder/layer_._{i}/attention/self/key/weight:0',
            f'encoder.layer.{i}.attention.self.key.bias': f'encoder/layer_._{i}/attention/self/key/bias:0',
            f'encoder.layer.{i}.attention.self.value.weight': f'encoder/layer_._{i}/attention/self/value/weight:0',
            f'encoder.layer.{i}.attention.self.value.bias': f'encoder/layer_._{i}/attention/self/value/bias:0',
            f'encoder.layer.{i}.attention.output.dense.weight': f'encoder/layer_._{i}/attention/output/dense/weight:0',
            f'encoder.layer.{i}.attention.output.dense.bias': f'encoder/layer_._{i}/attention/output/dense/bias:0',
            f'encoder.layer.{i}.attention.output.LayerNorm.weight': f'encoder/layer_._{i}/attention/output/layer_norm/gamma:0',
            f'encoder.layer.{i}.attention.output.LayerNorm.bias': f'encoder/layer_._{i}/attention/output/layer_norm/beta:0',
            f'encoder.layer.{i}.intermediate.dense.weight': f'encoder/layer_._{i}/intermediate/dense/weight:0',
            f'encoder.layer.{i}.intermediate.dense.bias': f'encoder/layer_._{i}/intermediate/dense/bias:0',
            f'encoder.layer.{i}.output.dense.weight': f'encoder/layer_._{i}/output/dense/weight:0',
            f'encoder.layer.{i}.output.dense.bias': f'encoder/layer_._{i}/output/dense/bias:0',
            f'encoder.layer.{i}.output.LayerNorm.weight': f'encoder/layer_._{i}/output/layer_norm/gamma:0',
            f'encoder.layer.{i}.output.LayerNorm.bias': f'encoder/layer_._{i}/output/layer_norm/beta:0',
        }
        base_mappings.update(layer_mappings)
    
    # Cr√©er le mapping final
    for pt_name, tf_name in base_mappings.items():
        if pt_name in pt_params and tf_name in tf_vars:
            weight_mapping[tf_vars[tf_name]] = pt_name
    
    return weight_mapping

def test_tflite_model(tflite_path, tokenizer, max_length):
    """
    Test rapide du mod√®le TFLite
    """
    try:
        # Charger le mod√®le TFLite
        interpreter = tf.lite.Interpreter(model_path=tflite_path)
        interpreter.allocate_tensors()
        
        # Obtenir les d√©tails des entr√©es et sorties
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        print(f"üìã Entr√©es du mod√®le TFLite :")
        for i, detail in enumerate(input_details):
            print(f"  {i}: {detail['name']} - Shape: {detail['shape']} - Type: {detail['dtype']}")
        
        print(f"üìã Sorties du mod√®le TFLite :")
        for i, detail in enumerate(output_details):
            print(f"  {i}: {detail['name']} - Shape: {detail['shape']} - Type: {detail['dtype']}")
        
        # Pr√©parer les donn√©es de test
        test_text = "This is a test sentence for model conversion."
        inputs = tokenizer(
            test_text,
            return_tensors="np",
            padding='max_length',
            truncation=True,
            max_length=max_length
        )
        
        # Faire une pr√©diction
        interpreter.set_tensor(input_details[0]['index'], inputs['input_ids'].astype(np.int32))
        interpreter.set_tensor(input_details[1]['index'], inputs['attention_mask'].astype(np.int32))
        
        interpreter.invoke()
        
        output_data = interpreter.get_tensor(output_details[0]['index'])
        predicted_class = np.argmax(output_data[0])
        
        print(f"‚úÖ Test r√©ussi ! Forme de sortie : {output_data.shape}")
        print(f"üìä Classe pr√©dite : {predicted_class}")
        print(f"üìä Scores (5 premiers) : {output_data[0][:5]}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du test : {str(e)}")

def convert_with_alternative_method(model_dir, tflite_output_path, max_length=128):
    """
    M√©thode alternative utilisant tf.keras directement
    """
    print("üîÑ Tentative avec m√©thode alternative (tf.keras)...")
    
    try:
        # Charger le mod√®le avec from_pretrained directement en TensorFlow
        print("üì• Chargement direct du mod√®le TensorFlow...")
        tf_model = TFAutoModelForSequenceClassification.from_pretrained(
            model_dir, 
            from_tf=False,  # Convertir depuis PyTorch
            local_files_only=True
        )
        tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)
        
        # Cr√©er un mod√®le Keras fonctionnel
        input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')
        attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')
        
        outputs = tf_model({'input_ids': input_ids, 'attention_mask': attention_mask})
        keras_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs.logits)
        
        # Conversion directe vers TFLite
        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS,
            tf.lite.OpsSet.SELECT_TF_OPS
        ]
        
        tflite_model = converter.convert()
        
        with open(tflite_output_path, "wb") as f:
            f.write(tflite_model)
        
        model_size = os.path.getsize(tflite_output_path) / (1024 * 1024)
        print(f"‚úÖ Conversion alternative r√©ussie !")
        print(f"üìÅ Mod√®le TFLite : {tflite_output_path}")
        print(f"üìè Taille du mod√®le : {model_size:.2f} MB")
        
        # Test du mod√®le
        test_tflite_model(tflite_output_path, tokenizer, max_length)
        
        return True
        
    except Exception as e:
        print(f"‚ùå M√©thode alternative √©chou√©e : {str(e)}")
        return False

if __name__ == "__main__":
    # === Configuration ===
    MODEL_DIR = "./model"
    TFLITE_OUTPUT_PATH = "./model.tflite"
    MAX_LENGTH = 128
    
    # V√©rifier que le dossier du mod√®le existe
    if not os.path.exists(MODEL_DIR):
        print(f"‚ùå Erreur : Le dossier {MODEL_DIR} n'existe pas !")
        print("Assurez-vous que le dossier contient :")
        print("  - model.safetensors (ou pytorch_model.bin)")
        print("  - config.json") 
        print("  - tokenizer.json")
        print("  - tokenizer_config.json")
        exit(1)
    
    # M√©thode principale
    print("üöÄ D√©marrage de la conversion SafeTensors -> TFLite")
    print("=" * 60)
    
    success = convert_safetensors_to_tflite(
        model_dir=MODEL_DIR,
        tflite_output_path=TFLITE_OUTPUT_PATH,
        max_length=MAX_LENGTH
    )
    
    # Si la m√©thode principale √©choue, essayer la m√©thode alternative
    if not success:
        print("\n" + "=" * 60)
        print("üîÑ Tentative avec m√©thode alternative...")
        success = convert_with_alternative_method(
            MODEL_DIR, 
            TFLITE_OUTPUT_PATH, 
            MAX_LENGTH
        )
    
    if success:
        print("\nüéâ Processus de conversion termin√© avec succ√®s !")
    else:
        print("\nüí• Toutes les m√©thodes de conversion ont √©chou√©.")
        print("Suggestions :")
        print("1. V√©rifiez que votre mod√®le est compatible")
        print("2. Essayez avec une version plus r√©cente de TensorFlow")
        print("3. Utilisez un opset ONNX plus ancien (11 ou 12)")